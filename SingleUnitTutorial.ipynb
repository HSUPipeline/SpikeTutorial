{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "answering-polymer",
   "metadata": {},
   "source": [
    "# Analyzing Single Neuron Activity\n",
    "\n",
    "In this tutorial we will examine single-neuron data collected from human patients.\n",
    "\n",
    "This tutorial was originally developed by \n",
    "[Salman Qasim](https://seqasim.wixsite.com/research),\n",
    "and has been updated by\n",
    "[Tom Donoghue](https://tomdonoghue.github.io/). \n",
    "\n",
    "### Requirements\n",
    "\n",
    "As well as standard scientific Python packages, this tutorial requires \n",
    "[pynwb](https://github.com/NeurodataWithoutBorders/pynwb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - standard scientific Python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Imports - single-unit related \n",
    "from pynwb import NWBHDF5IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn for plot aesthetics\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-yeast",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The predominant way in which individual neurons communicate with each other is through **action potentials**. These are rapid depolarizations across the neuronal cell membrane that culminate in a neuron releasing neurotransmitters that can do all sorts of things, like excite or inhibit action potentials in other neurons. Because action potentials are characterized by a large and fast change in membrane potential they can be recorded by microwires placed near the neuronal membrane, allowing for direct measurements of individual neuron activity.\n",
    "\n",
    "Quite differently from local field potentials, at any given moment in time a neuron is either spiking or not. That is, we can think of action potentials as discrete events, and a cell's activity over time as a series of discrete events - or a series of 'spike times'. From this, we can then compute a rate, or the number of spiking events across each unit time. \n",
    "\n",
    "When we look at neuronal spiking during a behavioral task, we are often looking for increases in the rate of spiking related to behavioral variables. This is thought to imply a relationship between the stimulus and the neuron's spiking, and is called **rate coding**. Here, we are going to cover some of the basics of analyzing single neuron responses in terms of rate coding.\n",
    "\n",
    "Note that in our examples here, we will be looking at data that has already been spike sorted, meaning we have isolated markers of neuron action potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS\n",
    "# Here, we will define a couple helper functions that we will use throughout the notebook.\n",
    "\n",
    "def compute_spike_rate(spikes):\n",
    "    \"\"\"Estimate spike rate from a vector of spike times, in seconds.\"\"\"\n",
    "    \n",
    "    return len(spikes) / (spikes[-1] - spikes[0])\n",
    "\n",
    "def get_spike_time_range(spikes, tmin, tmax):\n",
    "    \"\"\"Extract spike times for a particular time range.\"\"\"\n",
    "    \n",
    "    return spikes[np.squeeze(np.logical_and([spikes > tmin], [spikes < tmax]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-hunger",
   "metadata": {},
   "source": [
    "## First Dataset: Object Presentation\n",
    "\n",
    "The first dataset we will use is a an openly available dataset from human patients performing a recognition memory, provided by the \n",
    "[Rutishauser Lab](https://www.cedars-sinai.edu/research/labs/rutishauser.html). \n",
    "\n",
    "In this task, subjects are presented with pictures of objects, that they are later asked to recall. For our purposes, we will focus on the object presentation, and investigate whether we can find any neurons that seem to relate to visual stimulus presentation, especially of particular object types. \n",
    "\n",
    "For convenience, an example subject is included in this repository. The full dataset is available on \n",
    "[OSF repository](https://osf.io/cd6qp/), and described in this \n",
    "[paper](https://doi.org/10.1038/s41597-020-0415-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-curve",
   "metadata": {},
   "source": [
    "### Load NWB File\n",
    "\n",
    "The data that we are loading are in the [NWB](https://www.nwb.org/) format.\n",
    "\n",
    "In this tutorial, we won't go into much details on NWB files. If you are familiar with HDF5 files, NWB files are actually HDF5 files 'under the hood', with a specific schema for neural data. You can think of them a bit like a Python dictionary, optimized to store heterogeneous and potentially large data. The benefit of these files is that all the data from the experiment \n",
    "\n",
    "For more information on NWB files, see the \n",
    "[documentation](https://www.nwb.org/)\n",
    "and/or these\n",
    "[NWB examples](https://github.com/TomDonoghue/NWBExamples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datafile\n",
    "file_name = 'object_data.nwb'\n",
    "io = NWBHDF5IO('data/' + file_name, 'r')\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-subdivision",
   "metadata": {},
   "source": [
    "### Check Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many units in the current file\n",
    "n_units = len(nwbfile.units)\n",
    "print(n_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index for the unit of interest\n",
    "s_ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the waveform for the unit of interest\n",
    "waveform = nwbfile.units['waveform_mean_encoding'][s_ind, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the waveform of the unit of interest\n",
    "plt.plot(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spikes from a neuron of interest\n",
    "spikes = nwbfile.units.get_unit_spike_times(s_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a raster of a series of spikes\n",
    "plt.eventplot(spikes[0:50])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-license",
   "metadata": {},
   "source": [
    "### Interim Summary\n",
    "\n",
    "So far, we have loaded the data file and accessed some unit data for an example unit. \n",
    "\n",
    "Before proceeding, make sure you can access different neuron's, and different time ranges, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-taxation",
   "metadata": {},
   "source": [
    "### Event Information\n",
    "\n",
    "Next, in order to analyze the data with respect to task dynamics, we need to access the event data. \n",
    "\n",
    "In NWB, event information is stored as `intervals` type data (markers that denote intervals of interest in the dataset), within which the `trials` data stores structure information about the trial events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the encoding of event information in the NWB file\n",
    "nwbfile.intervals['trials']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access & check the behavioural information as a dataframe\n",
    "behav = nwbfile.trials.to_dataframe()\n",
    "behav.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-journal",
   "metadata": {},
   "source": [
    "In the dataframe above, we can see various events of interest for the task. \n",
    "\n",
    "For our purposes, we are going to focus on visual presentation of stimuli, and will seek to analyze these by stimulus category.\n",
    "\n",
    "The most relevant events for us are therefore:\n",
    "- `stim_on_time`, the time in the trial when the stimulus is presented\n",
    "- `stim_off_time`, the time in the trial when the stimulus is removed\n",
    "- `category_name`, the object category of the presented stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the available image conditions\n",
    "set(nwbfile.intervals['trials'].category_name.data[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stimulus presentation times\n",
    "stim_ons = nwbfile.intervals['trials']['stim_on_time'][:]\n",
    "stim_offs = nwbfile.intervals['trials']['stim_off_time'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stimulus onset times for a stimulus category of interest\n",
    "cond = 'phones'\n",
    "cond_stim_times = behav[behav.category_name == cond].stim_on_time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the stimulus onset times for our selected object category\n",
    "cond_stim_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-andorra",
   "metadata": {},
   "source": [
    "### Task Related Activity\n",
    "\n",
    "So far, we have explored the data, accessing the neural and event data. \n",
    "\n",
    "Next up, we can use our stimulus presentation times as events of interest, and examine spiking around these times, and try to examine if each neurons spiking is responsive to the presented stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_spikes(spikes, trial_times, window=1):\n",
    "    \"\"\"Extract spike times around event times of interest.\"\"\"\n",
    "    \n",
    "    trial_spikes = []\n",
    "    for trial_time in trial_times:\n",
    "        temp = get_spike_time_range(spikes, trial_time - window, trial_time + window)\n",
    "        trial_spikes.append(temp - trial_time)\n",
    "        \n",
    "    return trial_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spikes for a unit of interest\n",
    "s_ind = 0\n",
    "spikes = nwbfile.units.get_unit_spike_times(s_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect spikes by trial\n",
    "trial_spikes = get_trial_spikes(spikes, cond_stim_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the event-related raster plots\n",
    "plt.eventplot(trial_spikes);\n",
    "plt.vlines(0, -1, len(trial_spikes) + 1, color='green', alpha=0.75);\n",
    "plt.xlim([-1, 1])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-movie",
   "metadata": {},
   "source": [
    "#### Statistical Tests\n",
    "\n",
    "In the above, we have organized our data in order to visualize if there appear to be object related changes in neuron activity. \n",
    "\n",
    "In some cases, it might _look_ like there is a change in neuron activity, relating to the stimulus. In other cases, it might look like there is no change, and/or be unclear. In order to more systematically and quantitatively examine this question, we need to do some kind of statistics.\n",
    "\n",
    "How to statistically test neuron firing is a big topic in single-unit analyses. Conceptually, we want to test whether there is a significant change in firing rate, conditioned on some event of interest. \n",
    "\n",
    "Here, we will first start with a simple and conceptually consistent (though non-ideal) test for stimulus related firing - testing for a significant change in firing: a paired t-test on pre & post firing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct number of pre & post stim neurons\n",
    "n_pre, n_post = [], []\n",
    "for trial in trial_spikes:\n",
    "    n_pre.append(sum(trial < 0))\n",
    "    n_post.append(sum(trial > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a significant change in firing\n",
    "ttest_rel(n_pre, n_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-distinction",
   "metadata": {},
   "source": [
    "### Interim Summary\n",
    "\n",
    "In the above, we have explored a simple approach to examine if single-unit activity systematically relates to visual stimuli.\n",
    "\n",
    "If we explore the neurons and object categories, you should be able to find some neurons that seem to relate to object category! A neuron with clear encoding will show a visible change in firing, consistent across trials, relative to the stimulus onset time.\n",
    "\n",
    "We also used a simple statistical test to examine these results. This can help guide our analyses, however we should be careful that t-tests have assumptions about the data, that we didn't test! In fact, there are reasons to believe that our data probably don't follow the assumptions of a t-test. What this means in practice is that our test, and in particular the computed p-value, might not be appropriate. To more rigorously test these associations, we would want to use surrogate statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-drilling",
   "metadata": {},
   "source": [
    "### Object Dataset: Possible Extensions\n",
    "\n",
    "So far we explored a simple way to examine if there might be object related activity in the dataset, with simple visualizations and statistical approaches. \n",
    "\n",
    "There are many possible way to extend this analysis, that you may want to explore, including:\n",
    "- Properly examining significance of effects, for example with a permutation procedure\n",
    "- Generalizing the analysis across multiple neurons\n",
    "- Correcting for multiple comparisons for the number of neurons\n",
    "- Exploring further distinctions with the task, for example comparing the `learn` and `recog` phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-polish",
   "metadata": {},
   "source": [
    "## Second Dataset: Spatial Navigation\n",
    "\n",
    "We will primarily be analyzing data from the Train task, with which we will go over some simple single neuron analyses with respect to spatial position and memory. For reference for this task, see the \n",
    "[associated paper](https://www.nature.com/articles/s41593-019-0523-z).\n",
    "\n",
    "In this task, subjects move down a linear track while encoding and retrieving the locations of objects along the track. In this dataset, we will explore ploting neuronal spiking as a function of time and spatial position, and apply statistical methods for assessing significant changes in both domains. We will also examine measures the influence of memory cues and memory performance on spiking activity. \n",
    "\n",
    "The session file we will load is an example session from the train task, also organized into NWB format. This means that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data file\n",
    "file_name = 'spatial_data.nwb'\n",
    "io = NWBHDF5IO('data/' + file_name, 'r')\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many units in the current file\n",
    "n_units = len(nwbfile.units)\n",
    "print(n_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to access a unit of interest\n",
    "s_ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spikes for the unit of interest\n",
    "spikes = nwbfile.units.get_unit_spike_times(s_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check an example selection of spike times\n",
    "spikes[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-macro",
   "metadata": {},
   "source": [
    "### Descriptive Explorations\n",
    "\n",
    "In this dataset, let's start by exploring some descriptive measures of spiking activity.\n",
    "\n",
    "Note that as we can see from the spike times above, the spike times in this file are stored in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-salad",
   "metadata": {},
   "source": [
    "#### Firing Rate\n",
    "\n",
    "An initial, simple measure of unit activity is it's firing rate, which we can compute as the amount of spiking over some unit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the firing rate of the neuron\n",
    "#   Note that here we multiple by 1000 to get spikes per second\n",
    "fr = compute_spike_rate(spikes) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the firing rate of the unit of interest\n",
    "print('The firing rate is: {:2.2f}'.format(fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the firing rate for all neurons\n",
    "frs = [compute_spike_rate(nwbfile.units.get_unit_spike_times(ind)) * 1000 \\\n",
    "    for ind in range(n_units)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the firing rate for all neurons\n",
    "labels = ['U' + str(ind) for ind in range(len(frs))]\n",
    "_, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.bar(labels, frs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-judge",
   "metadata": {},
   "source": [
    "This tells you how active, in general, each neuron was during the recording session. \n",
    "\n",
    "Checking firing rates can also be used as quality code / pre-selection, since we might want to select neurons with a sufficiently high firing rate, and/or examine if any neurons have suspiciously high firing rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-doctrine",
   "metadata": {},
   "source": [
    "#### Inter-Spike Interval\n",
    "\n",
    "Another way to assess spiking activity is to look at the time interval between each spike (aka the **interspike interval**). \n",
    "\n",
    "By computing the inter-spike interval between all spikes, we can examine the distribution of interspike intervals.\n",
    "This gives us some information about the patterns of unit firing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ISI for a single neuron\n",
    "isi = np.diff(np.array(spikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the inter-spike intervals\n",
    "sns.distplot(isi)\n",
    "plt.xlabel('ISI (ms)')\n",
    "plt.ylabel('density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-gnome",
   "metadata": {},
   "source": [
    "#### Coefficient of Variation\n",
    "\n",
    "From the ISI distribution, you can compute the **coefficient of variation**. \n",
    "\n",
    "Note that the CV does not capture potential variability on longer time scales especially if there's drift in the neuron's mean firing rate over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-delivery",
   "metadata": {},
   "source": [
    "As an aside, another useful thing that ISI distribution can tell you is if the neuron you are looking at is a **bursty** neuron. By that, I mean a neuron that tends to fire a lot of action potentials in short bursts, rather than as isolated single spikes. As you can imagine, bursty neurons tend to violate Poisson assumptions and have ISI distributions that look a little more bimodal, with lots of spikes close together, and lots of bursts far apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coefficient of variation\n",
    "cv = np.std(isi) / np.mean(isi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the computed CV\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-matthew",
   "metadata": {},
   "source": [
    "### Data Representations\n",
    "\n",
    "Before we continue to examine the neural firing with respect to the task, let's consider and explore some different ways to represent the data.\n",
    "\n",
    "#### Spike Times\n",
    "\n",
    "So far, we have been using **spike times**, literally a list of the times at which a unit fired.\n",
    "\n",
    "#### Spike Trains\n",
    "\n",
    "Another way we can represent the data is a **spike train**, which is binary representation of 0's and 1's, in which each value represents whether the unit is spikig (1) or not (0). \n",
    "\n",
    "#### Continuous Firing Rates\n",
    "\n",
    "Note that if we are interested in rates, then we might be more interested in the overall rate of activity, than exact times at which spikes occur. One main reason to use spike trains is in order to compute continuous firing rates. By sweeping across windows in the soike train, and computing the firing rate within each time bin, we can compute a continuous measure of spike rate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-level",
   "metadata": {},
   "source": [
    "### Computing Spike Trains\n",
    "\n",
    "Let's now compute and look at some spike trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "bin_width_st = 1   # this means our time resolution is 1 ms\n",
    "st_sr = 1000       # This is our sampling rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-federation",
   "metadata": {},
   "source": [
    "Note that one thing we can also do now is to check and extract a time range specific to the task. Note that in the data collection, unit recordings might start before the task, and/or extend beyond the tak time. For the next step, let's define a spike train, selecting the spikes that happened during the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract start & stop time of the task from the event log\n",
    "start = nwbfile.intervals['trials'].start_time[0]\n",
    "end = nwbfile.intervals['trials'].stop_time[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spikes from during the task time\n",
    "#   First we find indices to keep, and then re-align time to 0\n",
    "keep_inds = np.where(np.logical_and(spikes>=start, spikes<=end))[0].astype(int)\n",
    "task_spikes = spikes[keep_inds] - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike times are in ms, but are sampled at 30 KHz \n",
    "#   This means we have to round them to ms resolution\n",
    "rounded_spike_times = np.round(task_spikes).astype(int)\n",
    "\n",
    "# Generate a spike train - a binary vector of 0's and 1's that is indexed by the spike times. \n",
    "n_vals = int(((np.ceil(end) - np.floor(start)) / bin_width_st) + 1)\n",
    "spiketrain = np.zeros(n_vals)\n",
    "\n",
    "# Add the spikes - note we have to subtract 1 for pythonic indexing\n",
    "spiketrain[rounded_spike_times - 1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spike train\n",
    "plt.plot(spiketrain[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-metro",
   "metadata": {},
   "source": [
    "#### Fano Factor\n",
    "\n",
    "A measure of spike time variability taking this long-term variability into account is the Fano factor, the ratio of the mean spike count and the variance the spike count within a time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compute the Fano Factor, which requires the spike train , during the task\n",
    "fano_factor = np.var(spiketrain) / np.mean(spiketrain)\n",
    "print('This cell has a Fano factor of {}'.format(fano_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-spider",
   "metadata": {},
   "source": [
    "### Computing Continous Firing Rates\n",
    "\n",
    "One of the key reasons to define a spike train is so that we can smooth it to get a continuous **estimate** of spiking. \n",
    "\n",
    "We can do this by convolving the spike train with a Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous firing rate parameters\n",
    "bin_width_gauss = 10                        # bin width, in ms\n",
    "gauss_sr = int(1000 / bin_width_gauss)      # gaussian sampling rate\n",
    "smoothing_width = 75                        # gaussian kernel, in ms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time stamps of the position data\n",
    "#   We can use this time vector to organize time bins, to match position data\n",
    "times = nwbfile.acquisition['position']['position'].timestamps[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the time bins \n",
    "times_offset = times - times[0]\n",
    "binned_time = np.arange(times_offset[0], times_offset[-1] + np.diff(times_offset)[0], bin_width_gauss)\n",
    "\n",
    "# Map the spike times to bins\n",
    "spkt = np.zeros(binned_time.shape)\n",
    "map_to_bins = np.digitize(task_spikes, binned_time)\n",
    "for i in map_to_bins:\n",
    "    if i > 0:\n",
    "        spkt[i - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window to filter in \n",
    "filt_window = np.arange(-1000, 1000, bin_width_gauss)\n",
    "\n",
    "# factor to convert the convolved firing rate into Hz (spikes/second)\n",
    "conv_rate_gaussian_fr = 1 / (bin_width_gauss / 1000) \n",
    "\n",
    "# Define the gaussian kernel\n",
    "gaussian_kernel = 1. / np.sqrt(2 * np.pi * smoothing_width ** 2) * np.exp(\n",
    "    -filt_window ** 2. / (2 * smoothing_width ** 2))\n",
    "\n",
    "# Normalize the kernel so that the area sums to 1\n",
    "gaussian_kernel =  gaussian_kernel / gaussian_kernel.sum()\n",
    "\n",
    "# Do the smoothing!\n",
    "spkt_conv = conv_rate_gaussian_fr * fftconvolve(spkt, gaussian_kernel, 'same') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot s of data\n",
    "f, (gauss, train) = plt.subplots(2, 1, figsize=[18,3])\n",
    "time_win = 100\n",
    "gauss.plot(spkt_conv[0:(time_win * gauss_sr)])\n",
    "train.plot(spiketrain[0:(time_win * st_sr)], linewidth=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-modeling",
   "metadata": {},
   "source": [
    "## Task Analysis\n",
    "\n",
    "Now let's look at how neural spiking changes in relation to task events.\n",
    "\n",
    "In general, the most useful tool for visualizing stimulus-related changes in the firing rate is the **peri-event rasters and histograms**. \n",
    "\n",
    "First, we have to do some data wrangling. The events here are not organized by trial/event, so we need to do that, then use our baselined spikes and the timesoffset field to parse the spiking the same way. \n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access & check the behavioural information as a dataframe\n",
    "behav = nwbfile.trials.to_dataframe()\n",
    "behav.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time for the responses per trial\n",
    "all_response_times = nwbfile.intervals['trials'].response_time.data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a time window of interest for around events\n",
    "time_window = [-500, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the spikes surrounding each stimulus \n",
    "spike_times_per_event = [] \n",
    "for response in all_response_times:\n",
    "    keep_event_inds = np.where(np.logical_and(task_spikes >= response + time_window[0], \n",
    "                                              task_spikes <= response + time_window[1]))[0].astype(int)\n",
    "    spike_times_per_event.append(task_spikes[keep_event_inds] - response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raster\n",
    "f, (raster, spike_hist) = plt.subplots(2, 1, figsize=[6,6])\n",
    "trial = 1\n",
    "for row in spike_times_per_event: \n",
    "    raster.vlines(row, trial, trial+1)\n",
    "    trial += 1\n",
    "raster.vlines(0, 0, trial)\n",
    "\n",
    "# Compute the histogram in 50 ms bins\n",
    "bin_width_ms = 50 \n",
    "n_bins = int(1000 / bin_width_ms)\n",
    "H, b = np.histogram(np.hstack(spike_times_per_event), n_bins)\n",
    "rate_factor = (bin_width_ms * len(nwbfile.intervals['trials']) / 1000)\n",
    "\n",
    "sns.distplot(np.hstack(spike_times_per_event), bins=n_bins, kde=False, ax=spike_hist)\n",
    "# Scale the y-axis so we are plotting firing rate in our bins, not just counts of spikes \n",
    "y_vals = spike_hist.get_yticks()\n",
    "spike_hist.set_yticklabels(['{:3.1f}'.format(x/rate_factor) for x in y_vals])\n",
    "spike_hist.vlines(0, 0, y_vals.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-commonwealth",
   "metadata": {},
   "source": [
    "### Analysis Notes\n",
    "\n",
    "Similar to our analysis of the previous dataset, we now have the question of how to do statistics for this kind of **peri-stimulus time histogram**? As we explored before, one option is to compute the mean firing rate pre & post and do a paired t-test.\n",
    "\n",
    "As we mentioned before, there are other ways to examine this question, including, for example, if we wanted to find particular time points in which there might be a change in firing rate. For example, to identify specific times of increased firing, we could generate surrogate PSTHs (i.e. 500 null observations per time bin), and compute a p-value for every bin to examine if any time bins in the non-shuffled real data exceed the empirical (shuffled) null. From there, one can do multiple comparisons correction across time bins to examine if there are any time bins that are significantly higher/lower than surrogates. \n",
    "\n",
    "We won't try and do this right now, but this is left as a potential extension of the analysis, and something we will follow up on in the extended work section at the end. Note that the the permutation statistics mentioned in the extended section will be very similar to what you might do for as PSTH. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-allocation",
   "metadata": {},
   "source": [
    "## Spatial Encoding Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-telephone",
   "metadata": {},
   "source": [
    "Now, let's examine neural activity as a function of position.\n",
    "\n",
    "Up until this point, we have been binning spike counts by **time**. Now it's time to bin spike counts by **position**.\n",
    "\n",
    "The easiest way to do this is to utilize pandas dataframe functionality, particularly the `cut` function to cut data into bins and the `groupby` function to apply a function to these split data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the position data from the file\n",
    "position = nwbfile.acquisition['position']['position']\n",
    "times = position.timestamps\n",
    "pos = position.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some position traces\n",
    "plt.plot(pos[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the environment into spatial bins, also collecting the bin edges\n",
    "n_spatial_bins = 20\n",
    "spatial_bins, bin_edges = pd.cut(pos[:], bins=n_spatial_bins, retbins=True,\n",
    "                                 include_lowest=True, labels=np.arange(n_spatial_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the bin assignment of each position\n",
    "spatial_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-shell",
   "metadata": {},
   "source": [
    "### Create a dataframe\n",
    "\n",
    "What we want to do next is examine if our single-neuron activity relates to task elements of interest - in this case spatial location and object presence. Note that now we have multiple behavioural features of interest, and we are going to have to take a different analysis strategy to examine how neural firing relates to multiple features of interest. \n",
    "\n",
    "To do this, we need a representation of the data that organized neural data, with measures of interest including space, objects, and neural activity. This requires some design choices, including, for example:\n",
    "- How do we encode our variable of interest?\n",
    "    - One could, for example, encode space in continuous or discrete ways\n",
    "- How do model the firing rate?\n",
    "    - One could, for example, encode firing rate as discrete firing rates, or a modeled as a Poisson process\n",
    "    \n",
    "For our purposes, we are going to discretize space into bins, use firing rates for our neural activity, and organize our data so that for each sampled time bin, we have spatial bin, trial object, and neural firing rate. \n",
    "\n",
    "To get started, for convenience, we can load a dataframe that has already been prepared this representation of the data. This dataframe includes the organized data for the 1st neuron in the dataset. For further analyses, and/or for other neurons, you can recreate these dataframes for each cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataframe of organized data for the current analysis\n",
    "data_df = pd.read_csv('data/spatial_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the loaded dataframe\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the actual spatially binned firing rate by averaging the firing rate over each bin \n",
    "spatially_binned_fr = data_df.groupby(by='spatial_bin').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the firing across spatial bins\n",
    "#   Note that seaborn does the groupby().mean() itself \n",
    "sns.lineplot(x='spatial_bin', y='fr', data=data_df, ci=68) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-transsexual",
   "metadata": {},
   "source": [
    "In the above, we can see how the spatial firing relates to spatial position across the linear track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the firing rate for each different object cued for memory \n",
    "sns.barplot(x='object_ID', y='fr', data=data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-january",
   "metadata": {},
   "source": [
    "In the above, we can see how the the average firing of the cell relates to which object is on the track.\n",
    "\n",
    "In the above plot, firing for each object is collapsed across trials (across both space and time). Next, we can examine the firing for the cell, split by object, across space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does spatial firing rate differ as a function of which object is cued for retrieval? \n",
    "diff_objects = data_df.object_ID.unique()\n",
    "fig, axes = plt.subplots(2, 2, figsize=[12, 8])\n",
    "\n",
    "for ind, obj in enumerate(axes.flatten()):\n",
    "    sns.lineplot(x='spatial_bin', y='fr', data=data_df[data_df.object_ID==diff_objects[ind]], ci=68, ax=obj)\n",
    "    obj.vlines(data_df[data_df.object_ID==diff_objects[ind]].object_bin, 0, obj.get_ylim()[-1])\n",
    "    obj.set_title(diff_objects[ind])\n",
    "    obj.set_ylim([0, 22])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-blake",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "\n",
    "Next, we need to find a way to do examine these patterns of firing statistically. To do so, with multiple behavioural features of interest (multiple locations & multiple objects), we are going to try and fit a model to see if & to what extent we can explain variance of the neural firing in terms of our events of interest.\n",
    "\n",
    "Though by no means the only way to do it, here we will take a fairly standard approach: using an ANOVA to investigate if binned spatial position and/or the trial object relate to cell firing. \n",
    "\n",
    "The goal of the model we want to fit is to statistically determine if this neuron shows significant spatial tuning, significant object tuning, or an interaction of the two. Let's discuss each of these possibilites:\n",
    "\n",
    "1) Significant spatial tuning: A neuron significantly increases it's firing in a particular location. We would call this a place cell. \n",
    "\n",
    "2) Significant object tuning: A neuron significantly increases it's firing when a particular object is cued for memory retrieval. \n",
    "\n",
    "3) Significant spatial x object: A neuron significantly increases it's firing in a particular location **as a function of the object cued for memory**. Put another way, the memory a person is cued to retrieve is affecting the spatial tuning of the neuron.\n",
    "\n",
    "To assess these possibilities in an individual neuron, we are going to do a simple 2-way ANOVA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formula of interest that we want to test\n",
    "formula = 'fr~C(spatial_bin) + C(object_ID) + C(spatial_bin)*C(object_ID)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = ols(formula, data_df).fit()\n",
    "aov_table = anova_lm(model, typ=2)\n",
    "\n",
    "# Grab variables of interest\n",
    "F_int = aov_table['F']['C(spatial_bin):C(object_ID)']\n",
    "F_pos = aov_table['F']['C(spatial_bin)']\n",
    "F_obj = aov_table['F']['C(object_ID)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model fit results\n",
    "aov_table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-shooting",
   "metadata": {},
   "source": [
    "Note that in the above, this neuron appears to have significant encoding of some features of interest!\n",
    "\n",
    "However, we must keep in mind that our data might violate normality assumptions for computing the significance of the test-statistic (F)!\n",
    "\n",
    "In order to more robustly evaluate the statistics, we should compute our own null distributions from surrogate data to assess significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-fetish",
   "metadata": {},
   "source": [
    "## Follow up Analyses\n",
    "\n",
    "So far, we have examined, for an single-cell, a first-pass analysis of if this cell's activity seems to relate to place and or object encoding. There is of course, much more we could explore here, and much we could generalize to analyze across multiple cells, sessions, etc, and to improve the statistical analyses. \n",
    "\n",
    "For this extended analyses, the goal is to further analyze human single neuron activity during a virtual-reality spatial memory task.\n",
    "\n",
    "#### 1) Explore PSTHs for other events\n",
    "\n",
    "First, try and write a general function to plot the raster and histogram at a user defined time window and bin length (for the histogram). Use it to plot the rasters and histograms for CueOn, CueOff, FeedbackOn and FeedbackOff. This function should use baselined spike times and baselined event times/trial as input. Plot the output for one neuron.\n",
    "\n",
    "#### 2) Explore smoothed firing rates for other events\n",
    "\n",
    "Write a similar function using the smoothed spiking activity instead of the PSTH. Keep in mind that the we computed the smoothed spiking activity at 100 Hz. Be sure to allow users to input width of smoothing kernel. You may want to write a separate function to smooth the firing rate, and call that within your function to plot the raster + smoothed firing rate. Plot the output for one neuron, with 3 different smoothing widths for your kernel. \n",
    "\n",
    "#### 3) Examine neural correlates of performance\n",
    "\n",
    "Calculate the mean firing rate and the error on every trial, and plot firing rate as a function of error. \n",
    "\n",
    "Hint: to do this, you are going to have to use the event information in the data file. Based on the object location, and the response location, you can calculate the behavioural error error per trial. Using the trial start and end times, you can also extract firing for these times of interest, and examine firing in trials as a function of error. \n",
    "\n",
    "BONUS: Is there a statistical relationship between firing rate and memory performance? \n",
    "\n",
    "#### 4) Generalize the ANOVA analysis\n",
    "\n",
    "Finally, see if you can generalize the analysis ANOVA analysis we did, both to generalize it across all cells, and also to add surrogate analysis to have more robust statistics.\n",
    "\n",
    "To do so, first you will need to write a helper function that organizes the data from the raw file into the kind of dataframe we used to fit the model. Following that dataframe, try and write a function to create this dataframe for each neuron. \n",
    "\n",
    "Then try and do some shuffling to determine if the F-statistic from the spatial ANOVA is significant. \n",
    "\n",
    "Hint: use np.roll() to circularly shift data in a pandas dataframe: \n",
    "`df.reindex(index=np.roll(df.index, shift))`.\n",
    "\n",
    "If you see any neurons with a significant interaction in the ANOVA, make a plot of the spatial firing rate split by object. If you see any neurons that do not have a significant interaction but do have a significant main effect of location, make a plot of the spatial firing rate over all trials. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
